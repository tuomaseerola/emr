{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MS_Seminar2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuomaseerola/emr/blob/main/audio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTO_mMkGJCci"
      },
      "source": [
        "# Music and Science - A Guide to Empirical Research in Music\n",
        "\n",
        "### [Tuomas Eerola](https://www.durham.ac.uk/staff/tuomas-eerola/), [Music and Science Lab]() at [Durham University]()\n",
        "\n",
        "A demo notebook with code examples in Python for Chapter 6 (Music Analysis).\n",
        "\n",
        "_Version 7/7/2022_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3LSkxYNS5nY"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt \n",
        "%matplotlib inline\n",
        "print(librosa.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2SD2rhaHDF9"
      },
      "source": [
        "## 2. Read audio files\n",
        "\n",
        "Here are a few audio files that come with Librosa. We can use any of them in the subsequent sections. Just remove the hashtag in front of the line to get that audio file.\n",
        "\n",
        "The following code also shows how to select only a _segment_ of an audio file. This is done by keywords `offset` and `duration`. Offset specifies where you want to start the segment (in seconds) and duration (also in seconds) is self-explanatory.\n",
        "\n",
        "**TASK:** Try out some of the audio examples yourself. To select one, remove the hashtag (#) from the beginning of the line. NB. Only one audio example should be run at a time, so make sure that the rest of the 'filename' commands have a # in front of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trn2ya5RNovU"
      },
      "source": [
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
        "#filename = librosa.ex('brahms')    # Hungarian Dance #5\n",
        "#filename = librosa.ex('choice')    # A short drum and bass loop\n",
        "#filename = librosa.ex('fishin')    # Karissa Hobbs - Let’s Go Fishin’ A folk/pop song with verse/chorus/verse structure and vocals\n",
        "#filename = librosa.ex('nutcracker')# Tchaikovsky - Dance of the Sugar Plum Fairy\n",
        "#filename = librosa.ex('vibeace')   # 60-second clip\n",
        "x, sr = librosa.load(filename, duration=20) # if you want to start the segment from a specific point, add offset = [2 for example] in the command\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(y=x,sr=sr)\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDVjKwaFfM"
      },
      "source": [
        "### Additional Info\n",
        "This is how to load example sound files from my Github pages. These are the instrument sounds used in Lecture 3 and some other examples (named example1b-d).\n",
        "**TASK:** import 2 different audio files (one at a time) and notice the different wave forms. Remember to put a # in front of the examples you are not using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cgjfvXZaMA3"
      },
      "source": [
        "import soundfile as sf\n",
        "import io\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/clar_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/trumpet_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/harp_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1d.wav\"\n",
        "url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1b.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1c.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/harpsichord_one_note.wav\"\n",
        "\n",
        "Y, samplerate = sf.read(io.BytesIO(urlopen(url).read()))\n",
        "#mono=librosa.to_mono(np.rot90(data))\n",
        "#Ym=librosa.resample(Y, samplerate, 22050)\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(y=Y,sr=samplerate,max_sr=100,color='purple')\n",
        "ipd.display(ipd.Audio(data=Y, rate=samplerate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwcceopAD2wz"
      },
      "source": [
        "## 3. Analyse loudness\n",
        "First extract RMS (Root Mean Square) energy and convert this into decibels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoSNdq0BHZP_"
      },
      "source": [
        "filename = librosa.ex('trumpet') #PROMPT: Plot the loudness for nutcracker and brahms excerpts. Which one has more dynamic changes?\n",
        "x, sr = librosa.load(filename, duration = 20)\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))\n",
        "\n",
        "rms=librosa.feature.rms(y=x)                 # Extra dynamics (RMS)\n",
        "db=librosa.amplitude_to_db(rms,ref=np.max)   # Convert into dB. Note that this is a relative measure (loudest is now 0) \n",
        "times = librosa.times_like(rms)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(times, db[0],color='black')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('dB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kql17pRbHbvd"
      },
      "source": [
        "## 4. Analyse pitch\n",
        "This is best done with a monophonic example, although librosa has algorithm to track all pitches as well. Here we extract the fundamental frequency of the trumpet solo using a probabilistic variant of the so-called YIN method. The variant is by Mauch and Dixon (2014) and the original technique was proposed by De Cheveigne and Kawahara (2002).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ7ukgJrHd4p"
      },
      "source": [
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording  #PROMPT: Run this code with the trumpet recording \n",
        "x, sr = librosa.load(filename)\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))    #code added to hear audio file\n",
        "\n",
        "# This is where the pyin algorithm is applied. fmin refers to the lower frequency threshold and fmax to higher frequence threshold.\n",
        "f0, voiced_flag, voiced_probs = librosa.pyin(x, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C6'))\n",
        "times = librosa.times_like(f0)\n",
        "\n",
        "# Plot the fundamental frequency\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(times,f0,'ro:', linewidth=2)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Hz')\n",
        "ax=plt.gca()\n",
        "ax.yaxis.set_major_formatter(librosa.display.LogHzFormatter())\n",
        "ax.set(yticks=[320,320+80,320+80*2,320+80*3,320+80*4,320+80*5])\n",
        "ax.set(ylim=[300, 800])\n",
        "ax.yaxis.set_major_formatter(librosa.display.LogHzFormatter())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGxCiVtTHfWc"
      },
      "source": [
        "## 5. Analyse Spectrum / Spectrogram\n",
        "- Try out the spectrogram with two different audio files from the available ones listed in Section 2 (trumpet sound vs example1d)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lazjn7vHjJ9"
      },
      "source": [
        "filename = librosa.ex('trumpet') #PROMPT: Try this out with the trumpet audio example. What is the spectrogram telling you?\n",
        "\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, sr = librosa.load(filename)  #for the example1d file, put a # in front of this line of code\n",
        "\n",
        "#Nfft = 512\n",
        "stft = np.abs(librosa.stft(x))\n",
        "freqs = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.specshow(librosa.amplitude_to_db(stft, ref=np.max), x_axis='time', y_axis='log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JY4Mgq1B1AT"
      },
      "source": [
        "## PROMPT: Look at the trumpet sound again. Take only the last note. Which frequency has the highest amplitude?\n",
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
        "x, sr = librosa.load(filename,offset=2.6,duration=0.75)     \n",
        "stft = np.abs(librosa.stft(x))\n",
        "f0, voiced_flag, voiced_probs = librosa.pyin(x, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "f=np.nanmedian(f0)        # Get the Hz of the fundamental frequency for nice labels\n",
        "n=librosa.hz_to_note(f)  # Convert Hz to note name\n",
        "print(n)\n",
        "x=np.arange(f,f*10,f)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "# collapse across time and plot a spectrum representation (energy across frequencies)\n",
        "Dmean=stft.mean(axis=1)/max(stft.mean(axis=1))\n",
        "plt.plot(freqs,Dmean,color='m')\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlim([300, 2000])\n",
        "plt.xticks(x);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omUNIRYyHhwQ"
      },
      "source": [
        "## 6. Extract onsets\n",
        "This estimates the strength of possible onsets and then detects the onsets that are stronger than a threshold defined in the algorithm. There is also a beat tracking that uses the onsets to estimate beat in the music."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oolz7CufHnOF"
      },
      "source": [
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
        "\n",
        "x, sr = librosa.load(filename,duration=20) \n",
        "\n",
        "o_env = librosa.onset.onset_strength(x, sr=sr)\n",
        "times = librosa.times_like(o_env, sr=sr)\n",
        "onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "#librosa.display.waveplot(librosa.amplitude_to_db(D, ref=np.max))\n",
        "plt.plot(times, o_env, label='Onset strength')\n",
        "plt.vlines(times[onset_frames], 0, o_env.max(), color='r', alpha=0.9,\n",
        "           linestyle='--', label='Onsets')\n",
        "plt.legend()\n",
        "\n",
        "# Sonify the detected beat events\n",
        "tempo, beats = librosa.beat.beat_track(y=x, sr=sr,trim=True)\n",
        "#y_beats = librosa.clicks(frames=onset_frames, sr=sr) # was beats\n",
        "y_beats = librosa.clicks(frames=beats, sr=sr) \n",
        "\n",
        "combined = (x[0:len(y_beats)]+y_beats)/2\n",
        "\n",
        "librosa.display.waveplot(y=y_beats,sr=sr)\n",
        "ipd.display(ipd.Audio(data=combined, rate=sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7zSbYC7NgNS"
      },
      "source": [
        "## PROMPT:Calculate the approximate tempo of the example\n",
        "\n",
        "y, sr = librosa.load(filename,duration=25) \n",
        "onset_env = librosa.onset.onset_strength(y, sr=sr)\n",
        "tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
        "print(np.round(tempo,1),'BPM')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}