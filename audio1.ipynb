{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MS_Seminar2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuomaseerola/emr/blob/main/audio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTO_mMkGJCci"
      },
      "source": [
        "## Music and Science – Seminar 2: Acoustic analysis\n",
        "\n",
        "[Tuomas Eerola](https://www.durham.ac.uk/staff/tuomas-eerola/), Durham University, Music Department, 2021.\n",
        "\n",
        "Audio examples adapted from the [FMP Notebooks](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C0/C0.html) by Meinhard Müller, which are part of [Fundamentals of Music Processing](https://www.audiolabs-erlangen.de/fau/professor/mueller/bookFMP).\n",
        "\n",
        "### How to use Colab\n",
        "- Code blocks can be run within a browser.\n",
        "- You can edit the code and try out what happens (change parameters etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3LSkxYNS5nY"
      },
      "source": [
        "#PROMPT: Press the play button to set up the technical system (import libraries etc.)\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt \n",
        "%matplotlib inline\n",
        "print(librosa.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJErDwSLJNz7"
      },
      "source": [
        "## 1. Create sine waves\n",
        "\n",
        "### 1.1. Define the properties of a sine wave"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWJUQL3I3z3"
      },
      "source": [
        "frequency = 2   # Frequency   #PROMPT: Change the values of the frequency, duration, amplitude, phase, and sampling rate to see how the output changes.\n",
        "duration = 2    # Duration of sound\n",
        "amplitude = 1.0 # Amplitude\n",
        "phase = 0.0     # Phase\n",
        "Fs = 100        # Sampling rate (per second)\n",
        "\n",
        "# This code creates the sine wave with the properties you detailed above\n",
        "num_samples = int(Fs * duration) \n",
        "t = np.arange(num_samples) / Fs\n",
        "x = amplitude * np.sin(2 * np.pi * (frequency * t - phase))\n",
        "\n",
        "# This code plots the result\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.plot(t, x, color='blue', linewidth=2.0, linestyle='-')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ft8hh4LL7Ab"
      },
      "source": [
        "### 1.2 Create a sine wave that you can listen to \n",
        "Try varying the parameters (frequency, phase, amplitude).\n",
        "\n",
        "**TASK:** Can you make a sine wave double the original frequency?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zooDsQiLqDD"
      },
      "source": [
        "frequency = 100  #PROMPT: Change the values of the frequency and phase and run the code. Notice the differences in the audio file and plot.\n",
        "duration = 0.25\n",
        "amplitude = 1.0\n",
        "phase = 0.0\n",
        "Fs = 22050\n",
        "\n",
        "num_samples = int(Fs * duration)\n",
        "t = np.arange(num_samples) / Fs\n",
        "x = amplitude * np.sin(2 * np.pi * (frequency * t - phase))\n",
        "\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.plot(t, x, color='blue', linewidth=2.0, linestyle='-')\n",
        "plt.xlim([0, duration])\n",
        "plt.ylim([-1, 1])\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.tight_layout()\n",
        "ipd.display(ipd.Audio(data=x, rate=Fs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9WIKgpjV-eD"
      },
      "source": [
        "### 1.3 Combine several sine waves to create a complex sound\n",
        "**TASK** - try altering the frequencies:\n",
        "  * Try to create a sound that has a missing fundamental frequency of 100Hz (lecture example had 800, 1000, and 1200 hz).\n",
        "  * Try to create a \"beating\" effect by having frequencies that are within 30 Hz. In which frequency and what differences creates the strongest sensation of beating? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze9JbgM1UDGL"
      },
      "source": [
        "# Combine several sine waves (here are three frequencies)\n",
        "frequency1 = 200 #PROMPT: Change the values of the 3 frequencies and listen to the output. Carry out the tasks above. \n",
        "frequency2 = 400\n",
        "frequency3 = 600\n",
        "duration = 1.0\n",
        "amplitude = 1.0\n",
        "phase = 0.0\n",
        "Fs = 22050\n",
        "\n",
        "num_samples = int(Fs * duration)\n",
        "t = np.arange(num_samples) / Fs\n",
        "x1 = amplitude * np.sin(2 * np.pi * (frequency1 * t - phase)) # 1st sine\n",
        "x2 = amplitude * np.sin(2 * np.pi * (frequency2 * t - phase)) # 2nd sine\n",
        "x3 = amplitude * np.sin(2 * np.pi * (frequency3 * t - phase)) # 3rd sine\n",
        "# Combine all three (sum and divide by 3 to keep the amplitude as original)\n",
        "x123=(x1+x2+x3)/3\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(t, x123, color='green')\n",
        "plt.xlim([0, 0.05])             # New element: Just show the first 50 ms\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.tight_layout()\n",
        "ipd.display(ipd.Audio(data=x123, rate=Fs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2SD2rhaHDF9"
      },
      "source": [
        "## 2. Read audio files\n",
        "\n",
        "Here are a few audio files that come with Librosa. We can use any of them in the subsequent sections. Just remove the hashtag in front of the line to get that audio file.\n",
        "\n",
        "The following code also shows how to select only a _segment_ of an audio file. This is done by keywords `offset` and `duration`. Offset specifies where you want to start the segment (in seconds) and duration (also in seconds) is self-explanatory.\n",
        "\n",
        "**TASK:** Try out some of the audio examples yourself. To select one, remove the hashtag (#) from the beginning of the line. NB. Only one audio example should be run at a time, so make sure that the rest of the 'filename' commands have a # in front of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trn2ya5RNovU"
      },
      "source": [
        "#filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
        "#filename = librosa.ex('brahms')    # Hungarian Dance #5\n",
        "#filename = librosa.ex('choice')    # A short drum and bass loop\n",
        "#filename = librosa.ex('fishin')    # Karissa Hobbs - Let’s Go Fishin’ A folk/pop song with verse/chorus/verse structure and vocals\n",
        "filename = librosa.ex('nutcracker')# Tchaikovsky - Dance of the Sugar Plum Fairy\n",
        "#filename = librosa.ex('vibeace')   # 60-second clip\n",
        "x, sr = librosa.load(filename, duration=20) # if you want to start the segment from a specific point, add offset = [2 for example] in the command\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(y=x,sr=sr)\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDVjKwaFfM"
      },
      "source": [
        "### Additional Info\n",
        "This is how to load example sound files from my Github pages. These are the instrument sounds used in Lecture 3 and some other examples (named example1b-d).\n",
        "**TASK:** import 2 different audio files (one at a time) and notice the different wave forms. Remember to put a # in front of the examples you are not using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cgjfvXZaMA3"
      },
      "source": [
        "import soundfile as sf\n",
        "import io\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/clar_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/trumpet_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/harp_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1d.wav\"\n",
        "url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1b.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1c.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/harpsichord_one_note.wav\"\n",
        "\n",
        "Y, samplerate = sf.read(io.BytesIO(urlopen(url).read()))\n",
        "#mono=librosa.to_mono(np.rot90(data))\n",
        "#Ym=librosa.resample(Y, samplerate, 22050)\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(y=Y,sr=samplerate,max_sr=100,color='purple')\n",
        "ipd.display(ipd.Audio(data=Y, rate=samplerate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwcceopAD2wz"
      },
      "source": [
        "## 3. Analyse loudness\n",
        "First extract RMS (Root Mean Square) energy and convert this into decibels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoSNdq0BHZP_"
      },
      "source": [
        "filename = librosa.ex('brahms') #PROMPT: Plot the loudness for nutcracker and brahms excerpts. Which one has more dynamic changes?\n",
        "x, sr = librosa.load(filename, duration = 20)\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))\n",
        "\n",
        "#url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1c.wav\"\n",
        "#x, samplerate = sf.read(io.BytesIO(urlopen(url).read()))\n",
        "#ipd.display(ipd.Audio(data=x, rate=samplerate))\n",
        "\n",
        "rms=librosa.feature.rms(y=x)                 # Extra dynamics (RMS)\n",
        "db=librosa.amplitude_to_db(rms,ref=np.max)   # Convert into dB. Note that this is a relative measure (loudest is now 0) \n",
        "times = librosa.times_like(rms)\n",
        "\n",
        "#db=librosa.feature.spectral_centroid(x)      # Show spectral centroid\n",
        "#times = librosa.times_like(rms)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(times, db[0],color='black')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('dB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kql17pRbHbvd"
      },
      "source": [
        "## 4. Analyse pitch\n",
        "This is best done with a monophonic example, although librosa has algorithm to track all pitches as well. Here we extract the fundamental frequency of the trumpet solo using a probabilistic variant of the so-called YIN method. The variant is by Mauch and Dixon (2014) and the original technique was proposed by De Cheveigne and Kawahara (2002).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ7ukgJrHd4p"
      },
      "source": [
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording  #PROMPT: Run this code with the trumpet recording \n",
        "x, sr = librosa.load(filename)\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))    #code added to hear audio file\n",
        "\n",
        "# This is where the pyin algorithm is applied. fmin refers to the lower frequency threshold and fmax to higher frequence threshold.\n",
        "f0, voiced_flag, voiced_probs = librosa.pyin(x, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C6'))\n",
        "times = librosa.times_like(f0)\n",
        "\n",
        "# Plot the fundamental frequency\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(times,f0,'ro:', linewidth=2)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Hz')\n",
        "ax=plt.gca()\n",
        "ax.yaxis.set_major_formatter(librosa.display.LogHzFormatter())\n",
        "ax.set(yticks=[320,320+80,320+80*2,320+80*3,320+80*4,320+80*5])\n",
        "ax.set(ylim=[300, 800])\n",
        "ax.yaxis.set_major_formatter(librosa.display.LogHzFormatter())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGxCiVtTHfWc"
      },
      "source": [
        "## 5. Analyse Spectrum / Spectrogram\n",
        "- Try out the spectrogram with two different audio files from the available ones listed in Section 2 (trumpet sound vs example1d)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lazjn7vHjJ9"
      },
      "source": [
        "filename = librosa.ex('trumpet') #PROMPT: Try this out with the trumpet audio example. What is the spectrogram telling you?\n",
        "\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1d.wav\" #PROMPT: Try out this other example. Which frequencies are the most prominent (lower ones or higher ones)?\n",
        "#x, samplerate = sf.read(io.BytesIO(urlopen(url).read()))\n",
        "\n",
        "x, sr = librosa.load(filename)  #for the example1d file, put a # in front of this line of code\n",
        "\n",
        "#Nfft = 512\n",
        "stft = np.abs(librosa.stft(x))\n",
        "freqs = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.specshow(librosa.amplitude_to_db(stft, ref=np.max), x_axis='time', y_axis='log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JY4Mgq1B1AT"
      },
      "source": [
        "## PROMPT: Look at the trumpet sound again. Take only the last note. Which frequency has the highest amplitude?\n",
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
        "x, sr = librosa.load(filename,offset=2.6,duration=0.75)     \n",
        "stft = np.abs(librosa.stft(x))\n",
        "f0, voiced_flag, voiced_probs = librosa.pyin(x, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "f=np.nanmedian(f0)        # Get the Hz of the fundamental frequency for nice labels\n",
        "n=librosa.hz_to_note(f)  # Convert Hz to note name\n",
        "print(n)\n",
        "x=np.arange(f,f*10,f)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "# collapse across time and plot a spectrum representation (energy across frequencies)\n",
        "Dmean=stft.mean(axis=1)/max(stft.mean(axis=1))\n",
        "plt.plot(freqs,Dmean,color='m')\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlim([300, 2000])\n",
        "plt.xticks(x);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omUNIRYyHhwQ"
      },
      "source": [
        "## 6. Extract onsets\n",
        "This estimates the strength of possible onsets and then detects the onsets that are stronger than a threshold defined in the algorithm. There is also a beat tracking that uses the onsets to estimate beat in the music."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oolz7CufHnOF"
      },
      "source": [
        "## PROMPT: Look at the Tchaikovsky example. Take the first 20 seconds (as duration). Afterwards, have a look at the 'brahms' example as well.\n",
        "filename = librosa.ex('brahms')\n",
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
        "\n",
        "x, sr = librosa.load(filename,duration=20) \n",
        "\n",
        "o_env = librosa.onset.onset_strength(x, sr=sr)\n",
        "times = librosa.times_like(o_env, sr=sr)\n",
        "onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "#librosa.display.waveplot(librosa.amplitude_to_db(D, ref=np.max))\n",
        "plt.plot(times, o_env, label='Onset strength')\n",
        "plt.vlines(times[onset_frames], 0, o_env.max(), color='r', alpha=0.9,\n",
        "           linestyle='--', label='Onsets')\n",
        "plt.legend()\n",
        "\n",
        "# Sonify the detected beat events\n",
        "tempo, beats = librosa.beat.beat_track(y=x, sr=sr,trim=True)\n",
        "#y_beats = librosa.clicks(frames=onset_frames, sr=sr) # was beats\n",
        "y_beats = librosa.clicks(frames=beats, sr=sr) \n",
        "\n",
        "combined = (x[0:len(y_beats)]+y_beats)/2\n",
        "\n",
        "librosa.display.waveplot(y=y_beats,sr=sr)\n",
        "ipd.display(ipd.Audio(data=combined, rate=sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7zSbYC7NgNS"
      },
      "source": [
        "## PROMPT:Calculate the approximate tempo of the nutcracker and brahms audio examples.\n",
        "filename = librosa.ex('nutcracker') \n",
        "y, sr = librosa.load(filename,duration=25) \n",
        "onset_env = librosa.onset.onset_strength(y, sr=sr)\n",
        "tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
        "print(np.round(tempo,1),'BPM')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Zsu5rYIKlb"
      },
      "source": [
        "## 7. Structure Discovery\n",
        "First extract a chromagram, then find the structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMS6rbcUICns"
      },
      "source": [
        "filename = librosa.ex('nutcracker')# Tchaikovsky - Dance of the Sugar Plum Fairy #PROMPT: Extract a chromagram for the 'nutcracker' - in which pitch class is the piece grounded?\n",
        "y, sr = librosa.load(filename,duration=60) # offset=2,duration=0.3\n",
        "\n",
        "C = librosa.feature.chroma_cqt(y=y, sr=sr, n_chroma=12)\n",
        "plt.figure(figsize=(18, 5))\n",
        "librosa.display.specshow(C, y_axis='chroma',x_axis='time')\n",
        "plt.title('CQT Chromagram')\n",
        "plt.show()\n",
        "ipd.display(ipd.Audio(data=y, rate=sr))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eboJnUGjrP2J"
      },
      "source": [
        "### Calculate recurrence matrix from Chromagram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoa3UX7SrQla"
      },
      "source": [
        "chroma_stack = librosa.feature.stack_memory(C, n_steps=8, delay=3) #PROMPT: What does the diagonal line in the matrix show us?\n",
        "rec = librosa.segment.recurrence_matrix(chroma_stack, mode='affinity', self=True)\n",
        "#rec_smooth = librosa.segment.path_enhance(rec, 51, window='hann', n_filters=7)\n",
        "plt.figure(figsize=(7, 7))\n",
        "img = librosa.display.specshow(rec, x_axis='time', y_axis='time')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gp3Ubd3rbzn"
      },
      "source": [
        "### Detect segment boundaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mQVg0Rgrc8l"
      },
      "source": [
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000) #PROMPT: Run this code to determine the segment boundaries of the nutcracker audio file. What do you think of the boundaries?\n",
        "bounds = librosa.segment.agglomerative(S, 8)\n",
        "bound_times = librosa.frames_to_time(bounds, sr=sr)\n",
        "#print(bound_times)\n",
        "\n",
        "ipd.display(ipd.Audio(data=y, rate=sr))\n",
        "S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "fig, ax = plt.subplots(figsize=(18, 5))\n",
        "librosa.display.specshow(S_dB, y_axis='mel', x_axis='time', ax=ax)\n",
        "ax.vlines(bound_times, 0, 8192, color='linen', linestyle='--',linewidth=2, alpha=0.9, label='Segment boundaries')\n",
        "ax.legend()\n",
        "ax.set(title='Power spectrogram')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjrwFunrwKR1"
      },
      "source": [
        "D = librosa.amplitude_to_db(np.abs(librosa.stft(y))) #PROMPT: What is the spectrogram showing?\n",
        "plt.figure(figsize=(12, 3))\n",
        "librosa.display.specshow(D, y_axis='log', x_axis='time')\n",
        "ipd.display(ipd.Audio(data=y, rate=sr))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}