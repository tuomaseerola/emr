{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Chapter 11 Code (Corpus analysis of genre classification)\n",
        "subtitle: How to Conduct Empirical Music Research\n",
        "author: Tuomas Eerola\n",
        "date: 3/20/2024\n",
        "copyright:\n",
        "  holder: Tuomas Eerola\n",
        "  year: 2024\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    toc: false\n",
        "    number-sections: true\n",
        "execute:\n",
        "  cache: true\n",
        "  enabled: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuomaseerola/emr/blob/main/Chapter11.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "A notebook with code examples in Python for _Chapter 11 (Music Corpus Studies)_ of _Routlegde_ book titled **[How to Conduct Empirical Music Research](https://github.com/tuomaseerola/emr)** by [Tuomas Eerola](https://www.durham.ac.uk/staff/tuomas-eerola/), [Music and Science Lab]() at [Durham University](https://www.durham.ac.uk) and [Music and Science Lab](https://musicscience.net) scheduled to be published in 2024.\n",
        "\n",
        "To run the code in your browser, open the file in Colab (click the icon \"Open in Colab\"). Alternatively, you can download the notebook and run it locally.\n",
        "\n",
        "> File `Chapter11.1.ipynb` | \n",
        "> Version `24/3/2024` |\n",
        "> [Back to Index](https://github.com/tuomaseerola/emr)\n",
        "\n",
        "<!--\n",
        "conda activate relative_mode \n",
        "quarto preview demo.qmd  \n",
        "\n",
        "I am currently working towards finishing this file.\n",
        "-->\n",
        "\n",
        "This notebook will carry a genre clustering from audio.\n",
        "\n",
        "# Setup\n",
        "\n",
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 Load dataset\n",
        "\n",
        "Let’s look at a classic genre categorization study and dataset by Tzanetakis & Cook (2002). The full data contains 100 audio excerpts from 10 different genres (1000 clips in total), but we are going to start with a smaller set to keep this light to run. It should be noted that the selection of the excerpts for this dataset for not particularly rigorous and represented the collection of music that George Tzanetakis had at his disposal at the time. And this dataset has some quirks and imperfections, but I think it is still a fun and illustrative example to explore.\n",
        "\n",
        "I first install `mirdata` library to the computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "#| label: install-mirdata\n",
        "import sys\n",
        "!{sys.executable} -m pip install mirdata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I then initiate the library and download samples needed. I only take 100 excerpts but you can take all 1000 excerpts by altering the script below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: download-samples\n",
        "import mirdata\n",
        "#print(mirdata.list_datasets())\n",
        "gtzan_genre = mirdata.initialize('gtzan_genre')\n",
        "gtzan = mirdata.initialize('gtzan_genre', version='mini')   # This is 100 excerpts\n",
        "#gtzan = mirdata.initialize('gtzan_genre')                  # This is 1000 excerpts (uncomment if you want to analyse the full data)\n",
        "gtzan.download()\n",
        "print('Downloaded',len(gtzan.track_ids),'tracks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at an example (`track ID 88`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: show-example\n",
        "ID = 88\n",
        "tracks = gtzan.load_tracks()\n",
        "ex = tracks[gtzan.track_ids[ID]]\n",
        "print([\"Genre:\", ex.genre, \"Name:\", ex.track_id, \"Tempo:\",ex.tempo,])\n",
        "print(ex.audio[1])\n",
        "plt.figure(figsize=(8, 2))\n",
        "librosa.display.waveshow(y = ex.audio[0], sr = ex.audio[1])\n",
        "ipd.display(ipd.Audio(data = ex.audio[0], rate = ex.audio[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract features\n",
        "\n",
        "Let's extract some features and use them to predict genres. We take some rhythm features, some timbral features, _MFCC_s (19 in total), and _chromas_ (12 in total). In the feature extraction, we calculate the features across the 30-second excerpt and then just take the mean to represent the feature.\n",
        "\n",
        "The summary of the track could be more sophisticated (one could take the median, and the standard deviation, for instance, or to avoid extreme values, or to take multiple measures from within each example, and using _bagging_ or _voting_ where separate clips are used in assessing the most likely genre. This latter goes close to [ensemble machine-learning techniques](https://en.wikipedia.org/wiki/Ensemble_learning))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: extract-features\n",
        "df = pd.DataFrame(columns = ['genre','bpm','rmse', 'spec_cent','spec_bw','rolloff','zcr','spec_ctr','mfcc1','mfcc2','mfcc3','mfcc4','mfcc5','mfcc6','mfcc7','mfcc8','mfcc9','mfcc10','mfcc11','mfcc12','mfcc13','mfcc14','mfcc15','mfcc16','mfcc17','mfcc18','mfcc19','chroma1','chroma2','chroma3','chroma4','chroma5','chroma6','chroma7','chroma8','chroma9','chroma10','chroma11','chroma12'])\n",
        "for track in tqdm(tracks):\n",
        "  ex = tracks[track]\n",
        "  y, sr = librosa.load(ex.audio_path)\n",
        "  chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "  rmse = librosa.feature.rms(y=y)\n",
        "  spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "  spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "  rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "  zcr = librosa.feature.zero_crossing_rate(y)\n",
        "  spec_ctr = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=512)\n",
        "  chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=512)\n",
        "  mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "  df.loc[len(df)] = [ex.genre,ex.tempo,np.mean(rmse),np.mean(spec_cent),np.mean(spec_bw),np.mean(rolloff),np.mean(zcr),np.mean(spec_ctr),np.mean(mfcc[1]),np.mean(mfcc[2]),np.mean(mfcc[3]),np.mean(mfcc[4]),np.mean(mfcc[5]),np.mean(mfcc[6]),np.mean(mfcc[7]),np.mean(mfcc[8]),np.mean(mfcc[9]),np.mean(mfcc[10]),np.mean(mfcc[11]),np.mean(mfcc[12]),np.mean(mfcc[13]),np.mean(mfcc[14]),np.mean(mfcc[15]),np.mean(mfcc[16]),np.mean(mfcc[17]),np.mean(mfcc[18]),np.mean(mfcc[19]),np.mean(chroma[0]),np.mean(chroma[1]),np.mean(chroma[2]),np.mean(chroma[3]),np.mean(chroma[4]),np.mean(chroma[5]),np.mean(chroma[6]),np.mean(chroma[7]),np.mean(chroma[8]),np.mean(chroma[9]),np.mean(chroma[10]),np.mean(chroma[11])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore features\n",
        "\n",
        "Let's look at some of features across genres. Are there differences in _dynamics_ or _brightness_?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: feature-exploration\n",
        "#| echo: false\n",
        "#print(df.shape)\n",
        "print(df.iloc[0:99,2:8].head(4))\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"ticks\", color_codes=True)\n",
        "fig, axes = plt.subplots(2,1, figsize=(8, 10))\n",
        "sns.boxplot(ax=axes[0], x=\"genre\", y=\"rmse\", data=df)\n",
        "sns.boxplot(ax=axes[1], x=\"genre\", y=\"spec_cent\", data=df,color='r')\n",
        "axes[0].set_title('RMSE Across Genres')\n",
        "axes[1].set_title('Spectral Centroid Across Genres')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature space\n",
        "\n",
        "How large is our feature space and do have features that are redundant, that is highly similar to each other? This can be easily explored by visualising the correlations between all features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: feature-space\n",
        "corr = df.iloc[0:99,1:27].corr() # Compute the correlation matrix\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))       # Generate a mask for the upper triangle\n",
        "f, ax = plt.subplots(figsize=(8, 8))                # Define matplotlib figure\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)  # Custom colormap\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.00, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}) # Draw the heatmap with the mask and correct aspect ratio\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification and machine-learning algorithms typically deal well with numerous features, but here we have only 100 observations and 39 variables, which is not a healthy proportion (too many variables compared to observations). Usually it is a good idea to have 10:1 or 15:1 or even 20:1 of observations to predictors. Based on the correlation matrix, what would you eliminate?\n",
        "\n",
        "For instance, all chroma features have high positive correlations and some of the timbral features seem to be related. Let's trim the selection as we have quite a little data when using the mini dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: feature-trimmed\n",
        "df_trimmed = df.iloc[:,0:22]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classify with the features\n",
        "\n",
        "We use all the features and relatively simple hierarchical classification tree model called [random forest](https://www.sciencedirect.com/topics/computer-science/random-forest-classifier). It creates a bunch of decision nodes based on the data by using a subset of the features and bootstrapping this process many times over. It is a robust technique and does not really whether the distribution are normal or not.\n",
        "\n",
        "Before running the model, there are three operations to introduce that are part of the good practice for model construction.\n",
        "\n",
        "## Cross-validation of the model\n",
        "\n",
        "We [cross-validate](https://scikit-learn.org/stable/modules/cross_validation.html) the model, which means that we split the data into _training_ and _testing sets_. We first train the model on the _training set_, which here is a randomly select 70% of the data. Once we have trained the model, we test it against the unseen data (_test set_, 30% of the data) to assess how the model performs.  This could be done by alterning the selection of the training and testing set and we could do this 10 times and average the results (this is called _k-fold cross-validation_).\n",
        "\n",
        "## Stratifying the sample\n",
        "\n",
        "When we randomly split the data into training and testing sets, we might want to _stratify_ the data according to genre, which makes sure that we have similar proportion of examples from each genre at both sets.\n",
        "\n",
        "## Normalize variables\n",
        "\n",
        "We also want to normalize the variables. This is not so crucial for the random forest model that we are going to use, but usually it is good idea to eliminate the differences the feature ranges have to the model. To normalize the variables, we turn them into z-scores, where the mean is 0 and standard deviation is 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: transform-data\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X = df_trimmed.drop('genre', axis = 1)\n",
        "Xn = preprocessing.normalize(X)\n",
        "y = df_trimmed['genre']\n",
        "\n",
        "test_size = 0.30 # taking 70:30 training and test set\n",
        "seed = 9  # Random numbmer seeding for reapeatability of the code\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=test_size, random_state=seed,stratify=y)\n",
        "\n",
        "RF = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0).fit(X_train, y_train)\n",
        "#RF.predict(X_test)\n",
        "#print(round(RF.score(X_test, y_test), 4))\n",
        "y_pred_test = RF.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we have the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: classification-rate\n",
        "print('Correct classification rate:',round(RF.score(X_test, y_test), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to answer this question, you should think what a model that predicts nonsense would achieve by chance? You could also check how this model compares to the work published by Tzanetakis. Finally, might want to consider what is level of accuracy expected from listeners and there might be even studies about this to give you a benchmark.\n",
        "\n",
        "# Analyse the model\n",
        "\n",
        "Let's look at this model in more detail and try to see which features are doing the most heavy lifting here and could we simplify the model and what kind of mistakes does the classification model make.\n",
        "\n",
        "## Visualise confusion matrix\n",
        "\n",
        "Let's explore what kind of mistakes the model makes. Confusion matrix is a useful way to visualise this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: visualise\n",
        "import seaborn as sns\n",
        "\n",
        "# Reshape\n",
        "matrix = confusion_matrix(y_test, y_pred_test)\n",
        "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Build the plot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(matrix, annot=True, annot_kws={'size':10},\n",
        "            cmap=plt.cm.Blues, linewidths=0.2)\n",
        "\n",
        "# Add labels to the plot\n",
        "class_names = RF.classes_ #np.unique(y_test)\n",
        "tick_marks = np.arange(len(class_names))\n",
        "tick_marks2 = tick_marks + 0.5\n",
        "plt.xticks(tick_marks+0.5, class_names, rotation=90)\n",
        "plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix for Random Forest Model')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importance\n",
        "\n",
        "Let's plot the feature importance from random forest classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: feature-importance\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "importance = RF.feature_importances_\n",
        "n = df_trimmed.columns[1:len(df.columns)]\n",
        "im = pd.DataFrame({'data': importance,'names': n})\n",
        "im2 = im.sort_values(by='data',ascending=False)\n",
        "# plot feature importance\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "#figure(figsize=(10, 5))\n",
        "plt.scatter(im2.names[0:9],im2.data[0:9],color='red')\n",
        "plt.plot(im2.names[0:9],im2.data[0:9])\n",
        "ax.set_title('10 strongest features')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot show the best 10 features and the first four seem to bring greater benefit to the model.\n",
        "\n",
        "\n",
        "## Simplify model\n",
        "\n",
        "What happens if we take the four best features and try building a simpler model with these features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "X2 = df_trimmed.filter(['mfcc8','rmse', 'spec_ctr', 'mfcc7'])\n",
        "\n",
        "test_size = 0.30 # taking 70:30 training and test set\n",
        "seed = 2022  # Random numbmer seeding for reapeatability of the code\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=test_size, random_state=seed,stratify=y)\n",
        "\n",
        "RF = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=seed).fit(X_train, y_train)\n",
        "RF.predict(X_test)\n",
        "# Make predictions for the test set\n",
        "y_pred_test = RF.predict(X_test)\n",
        "print(round(RF.score(X_test, y_test), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What do you think about the simplified model with 5 features? Is the model still good? You could look at the confusion to see what kind of mistakes the slimmer model starts to make.\n",
        "\n",
        "There is concept call _principle of parsimony_ or the idea behind that simpler models are more parsimonius than complex models, which stems from _Occam’s razor_. There are several statistical measures that assess the model fit and parsimoniousness ([Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) etc.). We are not entering into those calculations here but usually it is better to have a simple model and compromise the model accuracy a little bit than to gain few points in accuracy but having a complex model.\n",
        "\n",
        "## Summary\n",
        "\n",
        "There numerous other algorithms to classify the materials, SVMs ([Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)), [K-nearest neighbour models (KNNs)](https://scikit-learn.org/stable/modules/neighbors.html), [Neural networks](https://scikit-learn.org/stable/modules/neural_networks_supervised.html), and many others.\n",
        "\n",
        "We could have focussed more on features, their calculation, the summary measures, and subsets, but overall we achieved a good success with a small set of features. We have to remember that this is a mini-version of the original dataset. You are welcome to try how the full dataset would improve the results.\n",
        "\n",
        "This process is pretty generic for all kinds of classification tasks, so the same procedure could be applied to prediction _emotion categories_, _meter_, _instrumentation_ and other properties of music.\n",
        "\n",
        "# References\n",
        "\n",
        "- Tzanetakis, G. & Cook, P. (2002). Musical genre classification of audio signals. _IEEE Transactions on Speech and Audio Processing, 10(5)_, 293-302 [doi:10.1109/TSA.2002.800560](https://ieeexplore.ieee.org/document/1021072).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}